---
layout: post
title:  Linux之TCPIP内核参数优化
categories: [网络]
tags: [Linux, socket,]
description: ""
---

### /proc/sys/net目录

推荐先阅读
[《TCP连接的状态与关闭方式，及其对Server与Client的影响》](http://www.cnblogs.com/fczjuever/archive/2013/04/05/3000680.html)、
[《Windows系统下的TCP参数优化》](http://www.cnblogs.com/fczjuever/archive/2013/04/05/3000697.html)，
以了解TCP优化的相关知识。

&emsp;&emsp;&emsp;&emsp;所有的TCP/IP参数都位于/proc/sys/net目录下（请注意，对/proc/sys/net
目录下内容的修改都是临时的，任何修改在系统重启后都会丢失），例如下面这些重要的参数：


参数（路径+文件） | 描述 | 默认值 | 优化值 
----- | ----- | ----- | -----  
/proc/sys/net/core/rmem_default | 默认的TCP数据接收窗口大小（字节）。 | 229376 | 256960 
/proc/sys/net/core/rmem_max     | 最大的TCP数据接收窗口（字节）。     | 131071 | 513920
/proc/sys/net/core/wmem_default | 默认的TCP数据发送窗口大小（字节）。 | 229376 | 256960
/proc/sys/net/core/wmem_max     | 最大的TCP数据发送窗口（字节）。     | 131071 | 513920
/proc/sys/net/core/netdev_max_backlog | 在每个网络接口接收数据包的速率比内核处理这些包的速率快时，允许送到队列的数据包的最大数目。          | 1000   | 2000
/proc/sys/net/core/somaxconn   | 定义了系统中每一个端口最大的监听队列的长度，这是个全局的参数。| 128 |2048
/proc/sys/net/core/optmem_max  | 表示每个套接字所允许的最大缓冲区的大小。| 20480 | 81920
/proc/sys/net/ipv4/igmp_max_memberships | 主机上最多有多少个igmp (多播)套接字进行监听 | 100 |
/proc/sys/net/ipv4/inet_peer_maxttl | entries的最大生存期。在pool没有内存压力的情况下(比如，pool中entries的数量很少的时候)，未使用的entries经过一段时间就会过期。以jiffies计 | 600 |
/proc/sys/net/ipv4/inet_peer_minttl | entries的最小生存期。应该不小于汇聚端分片的生存期。当pool的大小不大于inet_peer_threshold时，这个最小生存期必须予以保证。以jiffies计 | 120 |
/proc/sys/net/ipv4/inet_peer_threshold | The approximate size of the INET peer storage. Starting from this threshold entries will be thrown aggressively. This threshold also determines entries' time-to-live anｄ time intervals between garbage collection passes. More entries, less time-to-live, less GC interval | 65664 | 
/proc/sys/net/ipv4/ip_default_ttl | 数据包的生存期。设置为64是安全的。如果你的网络规模巨大就提高这个值。不要因为好玩而这么做——那样会产生有害的路由环路。实际上，在很多情况下你要考虑能否减小这个值 | 64 | 
/proc/sys/net/ipv4/ip_dynaddr | 如果你有一个动态地址的自动拨号接口，就得设置它。当你的自动拨号接口激活的时候，本地所有没有收到答复的TCP套接字会重新绑定到正确的地址上。这可以解决引发拨号的套接字本身无法工作，重试一次却可以的问题 | 0 |
/proc/sys/net/ipv4/ip_forward | 内核是否转发数据包。| 0 | 
/proc/sys/net/ipv4/ip_local_port_range | 表示TCP/UDP协议允许使用的本地端口号 | 32768  61000 | 1024  65000
/proc/sys/net/ipv4/ip_no_pmtu_disc | 如果你想禁止“沿途MTU发现”就设置它。“沿途MTU发现”是一种技术，可以在传输路径上检测出最大可能的MTU值。参见Cookbook一章中关于“沿途MTU发现”的内容。 | 0 |
/proc/sys/net/ipv4/ipfrag_high_thresh | 用 于IP分片汇聚的最大内存用量。分配了这么多字节的内存后，一旦用尽，分片处理程序就会丢弃分片。When ipfrag_high_thresh bytes of memory is allocated for this purpose, the fragment handler will toss packets until ipfrag_low_thresh is reached. | |
/proc/sys/net/ipv4/ipfrag_low_thresh | 用于IP分片汇聚的最小内存用量。 | |
/proc/sys/net/ipv4/ip_nonlocal_bind | 如果你希望你的应用程序能够绑定到不属于本地网卡的地址上时，设置这个选项。如果你的机器没有专线连接(甚至是动态连接)时非常有用，即使你的连接断开，你的服务也可以启动并绑定在一个指定的地址上 | | 
/proc/sys/net/ipv4/tcp_abort_on_overflow | 一个布尔类型的标志，控制着当有很多的连接请求时内核的行为。启用的话，如果服务超载，内核将主动地发送RST包 | |
/proc/sys/net/ipv4/tcp_max_orphans | 系统中最多有多少个TCP套接字不被关联到任何一个用户文件句柄上。如果超过这个数字，孤儿连接将即刻被复位并打印出警告信息。这个限制仅仅是为了防止简单的DoS攻击，你绝对不能过分依靠它或者人为地减小这个值，更应该增加这个值(如果增加了内存之后)。This limit exists only to prevent simple DoS attacks, you _must_ not rely on this oｒ lower the limit artificially, but rather increase it (probably, after increasing installed memory), if network conditions require more than default value, anｄ tune network services to linger anｄ kill such states more aggressively. 让我再次提醒你：每个孤儿套接字最多能够吃掉你64K不可交换的内存 | |
/proc/sys/net/ipv4/tcp_orphan_retries | 本端试图关闭TCP连接之前重试多少次。缺省值是7，相当于50秒~16分钟(取决于RTO)。如果你的机器是一个重载的WEB服务器，你应该考虑减低这个值，因为这样的套接字会消耗很多重要的资源。参见tcp_max_orphans | |
/proc/sys/net/ipv4/tcp_max_tw_buckets | 系统同时保持timewait套接字的最大数量。如果超过这个数字，time-wait套接字将立刻被清除并打印警告信息。这个限制仅仅是为了防止简单的 DoS攻击，你绝对不能过分依靠它或者人为地减小这个值，如果网络实际需要大于缺省值，更应该增加这个值(如果增加了内存之后)。 | | 
/proc/sys/net/ipv4/tcp_retrans_collapse | 为兼容某些糟糕的打印机设置的“将错就错”选项。再次发送时，把数据包增大一些，来避免某些TCP协议栈的BUG。| |
/proc/sys/net/ipv4/tcp_retries1       | 在认定出错并向网络层提交错误报告之前，重试多少次。缺省设置为RFC规定的最小值：3，相当于3秒~8分钟（取决于RIO） | |
/proc/sys/net/ipv4/tcp_retries2       | 在杀死一个活动的TCP连接之前重试多少次。RFC 1122规定这个限制应该长于100秒。这个值太小了。缺省值是15，相当于13~30分钟（取决于RIO）。| |
/proc/sys/net/ipv4/tcp_rfc1337        | 这个开关可以启动对于在RFC1337中描述的“tcp的time-wait暗杀危机”问题的修复。启用后，内核将丢弃那些发往time-wait状态TCP套接字的RST包。却省为0。 | |
/proc/sys/net/ipv4/tcp_sack  | 特别针对丢失的数据包使用选择性ACK，这样有助于快速恢复 |  | 
/proc/sys/net/ipv4/tcp_stdurg | 使用TCP紧急指针的主机需求解释。因为绝大多数主机采用BSD解释，所以如果你在Linux上打开它，可能会影响它与其它机器的正常通讯。缺省是FALSE | |
/proc/sys/net/ipv4/tcp_syn_retries | 在内核放弃建立连接之前发送SYN包的数量 | |
/proc/sys/net/ipv4/tcp_synack_retries | 为了打开对端的连接，内核需要发送一个SYN并附带一个回应前面一个SYN的ACK。也就是所谓三次握手中的第二次握手。这个设置决定了内核放弃连接之前发送SYN+ACK包的数量。| |
/proc/sys/net/ipv4/tcp_mem     | 确定TCP栈应该如何反映内存使用，每个值的单位都是内存页（通常是4KB）。第一个值是内存使用的下限；第二个值是内存压力模式开始对缓冲区使用应用压力的上限；第三个值是内存使用的上限。在这个层次上可以将报文丢弃，从而减少对内存的使用。对于较大的BDP可以增大这些值（注意，其单位是内存页而不是字节）。|  94011  125351  188022 |131072  262144  524288
/proc/sys/net/ipv4/tcp_rmem    | 为自动调优定义socket使用的内存。第一个值是为socket接收缓冲区分配的最少字节数；第二个值是默认值（该值会被rmem_default覆盖），缓冲区在系统负载不重的情况下可以增长到这个值；第三个值是接收缓冲区空间的最大字节数（该值会被rmem_max覆盖）。| 4096  87380  4011232 | 8760  256960  4088000
/proc/sys/net/ipv4/tcp_wmem    | 为自动调优定义socket使用的内存。第一个值是为socket发送缓冲区分配的最少字节数；第二个值是默认值（该值会被wmem_default覆盖），缓冲区在系统负载不重的情况下可以增长到这个值；第三个值是发送缓冲区空间的最大字节数（该值会被wmem_max覆盖）。| 4096  16384  4011232 | 8760  256960  4088000
/proc/sys/net/ipv4/tcp_keepalive_time | TCP发送keepalive探测消息的间隔时间（秒），用于确认TCP连接是否有效。| 7200 | 1800
/proc/sys/net/ipv4/tcp_keepalive_intvl | 探测消息未获得响应时，重发该消息的间隔时间（秒）。| 75 | 30
/proc/sys/net/ipv4/tcp_keepalive_probes | 在认定TCP连接失效之前，最多发送多少个keepalive探测消息。| 9 | 3
/proc/sys/net/ipv4/tcp_sack | 启用有选择的应答（1表示启用），通过有选择地应答乱序接收到的报文来提高性能，让发送者只发送丢失的报文段，（对于广域网通信来说）这个选项应该启用，但是会增加对CPU的占用。| 1 | 1
/proc/sys/net/ipv4/tcp_fack | 启用转发应答，可以进行有选择应答（SACK）从而减少拥塞情况的发生，这个选项也应该启用。| 1 | 1
/proc/sys/net/ipv4/tcp_timestamps | TCP时间戳（会在TCP包头增加12个字节），以一种比重发超时更精确的方法（参考RFC 1323）来启用对RTT 的计算，为实现更好的性能应该启用这个选项。| 1 | 1
/proc/sys/net/ipv4/tcp_window_scaling | 启用RFC 1323定义的window scaling，要支持超过64KB的TCP窗口，必须启用该值（1表示启用），TCP窗口最大至1GB，TCP连接双方都启用时才生效。 | 1 | 1
/proc/sys/net/ipv4/tcp_syncookies  | 表示是否打开TCP同步标签（syncookie），内核必须打开了CONFIG_SYN_COOKIES项进行编译，同步标签可以防止一个套接字在有过多试图连接到达时引起过载。| 1 | 1
/proc/sys/net/ipv4/tcp_tw_reuse    | 表示是否允许将处于TIME-WAIT状态的socket（TIME-WAIT的端口）用于新的TCP连接 。| 0 | 1
/proc/sys/net/ipv4/tcp_tw_recycle  | 能够更快地回收TIME-WAIT套接字。| 0 | 1
/proc/sys/net/ipv4/tcp_fin_timeout | 对于本端断开的socket连接，TCP保持在FIN-WAIT-2状态的时间（秒）。对方可能会断开连接或一直不结束连接或不可预料的进程死亡。| 60 | 30
/proc/sys/net/ipv4/tcp_max_syn_backlog | 对于还未获得对方确认的连接请求，可保存在队列中的最大数目。如果服务器经常出现过载，可以尝试增加这个数字。| 2048 | 2048
/proc/sys/net/ipv4/tcp_low_latency     | 允许TCP/IP栈适应在高吞吐量情况下低延时的情况，这个选项应该禁用。| 0 | 
/proc/sys/net/ipv4/tcp_westwood        | 启用发送者端的拥塞控制算法，它可以维护对吞吐量的评估，并试图对带宽的整体利用情况进行优化，对于WAN 通信来说应该启用这个选项。| 0 |
/proc/sys/net/ipv4/tcp_bic             | 为快速长距离网络启用Binary Increase Congestion，这样可以更好地利用以GB速度进行操作的链接，对于WAN通信应该启用这个选项。| 1 |

 


{% highlight ruby %}
文档信息
--------------
* 版权声明：自由转载-非商用
* 转载: [Markdown简明教程]
{% endhighlight %}

[Linux之TCPIP内核参数优化](http://www.cnblogs.com/fczjuever/archive/2013/04/17/3026694.html)
[proc/sys/net/ipv4/下各项的意义](http://lijichao.blog.51cto.com/67487/308509)

[jekyll]:      http://jekyllrb.com
[jekyll-gh]:   https://github.com/jekyll/jekyll
[jekyll-help]: https://github.com/jekyll/jekyll-help
